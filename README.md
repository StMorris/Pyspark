# Pyspark

### To support Python with Spark, Apache Spark community released a tool, PySpark. PySpark is a Python API for Apache Spark.

Apache spark is designed as a high performance general-purpose competition engine, it works by distributing its work load across different nodes in a cluster. It provides high level APIs in Java Scala Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of high level tools, including *SPARK SQL* for sql and structured data processing, MLlib for machine learning, GraphX for graph processing and spark streaming.

Apache Spark is an analytical processing engine for large scale powerful distributed data processing and machine learning applications. 
Apache Spark is written in Scala programming language, PySpark lets us work with RDDs in Python programming language, thanks to a library called Py4j that they are able to achieve this. Py4J is a Java library that is integrated within PySpark and allows python to dynamically interface with JVM (Java virtual machine) objects, hence to run PySpark you also need Java to be installed along with Python, and Apache Spark.
PySpark supports most of Sparkâ€™s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.

PySpark is very well used in Data Science and Machine Learning community due to its efficient processing of large datasets.
